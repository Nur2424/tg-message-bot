{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Telegram Bot — Data Preprocessing\n",
        "This notebook loads a Telegram JSON export, flattens messages, filters junk, cleans text, exports a clean dataset, and optionally creates a balanced sample for manual labeling.\n",
        "\n",
        "**Important:** Do **NOT** commit your raw Telegram data to GitHub. Use `.gitignore` to exclude `data/` and any private folders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 1 — Imports + file path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# ✅ change filename to your real export json name\n",
        "JSON_PATH = Path(\"../data/raw/result.json\")\n",
        "\n",
        "assert JSON_PATH.exists(), f\"File not found: {JSON_PATH.resolve()}\"\n",
        "print(\"Using:\", JSON_PATH.resolve())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 2 — Load JSON + inspect structure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "with JSON_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(\"Top-level type:\", type(data))\n",
        "\n",
        "if isinstance(data, dict):\n",
        "    print(\"Top-level keys (first 30):\", list(data.keys())[:30])\n",
        "elif isinstance(data, list):\n",
        "    print(\"List length:\", len(data))\n",
        "    print(\"First element type:\", type(data[0]) if len(data) else None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 3 — Helper: Telegram `text` to string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def tg_text_to_str(text_field):\n",
        "    \"\"\"\n",
        "    Telegram JSON export: 'text' can be a string OR a list of parts.\n",
        "    This returns a single plain string.\n",
        "    \"\"\"\n",
        "    if text_field is None:\n",
        "        return \"\"\n",
        "    if isinstance(text_field, str):\n",
        "        return text_field\n",
        "    if isinstance(text_field, list):\n",
        "        out = []\n",
        "        for part in text_field:\n",
        "            if isinstance(part, str):\n",
        "                out.append(part)\n",
        "            elif isinstance(part, dict):\n",
        "                out.append(part.get(\"text\", \"\"))\n",
        "        return \"\".join(out)\n",
        "    return str(text_field)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 4 — Extract chats/messages blocks (supports common Telegram formats)\n",
        "If you get `Chats found: 0`, run Cell 2 and share the keys you see; Telegram exports sometimes differ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def extract_message_blocks(data):\n",
        "    \"\"\"\n",
        "    Returns list of blocks: [{\"chat_name\": ..., \"messages\": [...]}, ...]\n",
        "    Supports:\n",
        "      1) {\"name\": ..., \"messages\":[...]}   (single chat)\n",
        "      2) {\"chats\": {\"list\":[{name, messages}, ...]}}  (full export)\n",
        "      3) {\"messages\":[...]} (fallback)\n",
        "    \"\"\"\n",
        "    blocks = []\n",
        "\n",
        "    # case 1: single chat export\n",
        "    if isinstance(data, dict) and isinstance(data.get(\"messages\"), list):\n",
        "        blocks.append({\n",
        "            \"chat_name\": data.get(\"name\", data.get(\"title\", \"unknown_chat\")),\n",
        "            \"messages\": data[\"messages\"]\n",
        "        })\n",
        "        return blocks\n",
        "\n",
        "    # case 2: full export with chats.list\n",
        "    if isinstance(data, dict):\n",
        "        chats = data.get(\"chats\")\n",
        "        if isinstance(chats, dict):\n",
        "            chat_list = chats.get(\"list\")\n",
        "            if isinstance(chat_list, list):\n",
        "                for chat in chat_list:\n",
        "                    if isinstance(chat, dict) and isinstance(chat.get(\"messages\"), list):\n",
        "                        blocks.append({\n",
        "                            \"chat_name\": chat.get(\"name\", chat.get(\"title\", \"unknown_chat\")),\n",
        "                            \"messages\": chat[\"messages\"]\n",
        "                        })\n",
        "                if blocks:\n",
        "                    return blocks\n",
        "\n",
        "    # no match\n",
        "    return blocks\n",
        "\n",
        "blocks = extract_message_blocks(data)\n",
        "print(\"Chats found:\", len(blocks))\n",
        "print(\"Example chat:\", blocks[0][\"chat_name\"] if blocks else \"NONE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 5 — Flatten messages into DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "rows = []\n",
        "\n",
        "for block in blocks:\n",
        "    chat_name = block[\"chat_name\"]\n",
        "    for m in block[\"messages\"]:\n",
        "        if not isinstance(m, dict):\n",
        "            continue\n",
        "\n",
        "        text_raw = tg_text_to_str(m.get(\"text\"))\n",
        "\n",
        "        rows.append({\n",
        "            \"chat_name\": chat_name,\n",
        "            \"msg_id\": m.get(\"id\"),\n",
        "            \"date\": m.get(\"date\"),\n",
        "            \"from_name\": m.get(\"from\"),\n",
        "            \"from_id\": m.get(\"from_id\"),\n",
        "            \"type\": m.get(\"type\"),\n",
        "            \"text_raw\": text_raw,\n",
        "            \"reply_to_msg_id\": m.get(\"reply_to_message_id\"),\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "print(\"Rows:\", len(df))\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 6 — Parse date + basic checks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
        "\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "print(\"\\nType counts:\\n\", df[\"type\"].value_counts(dropna=False).head(10))\n",
        "print(\"\\nSample texts:\\n\", df[\"text_raw\"].dropna().astype(str).head(5).to_list())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 3 — Filter junk (service + empty + too-short noise)\n",
        "Keeps messages that are **not** service messages, have non-empty text, and have >=2 words (or contain '?')."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 7 — Filter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "df[\"text_raw\"] = df[\"text_raw\"].fillna(\"\").astype(str)\n",
        "\n",
        "df[\"is_empty_text\"] = df[\"text_raw\"].str.strip().eq(\"\")\n",
        "df[\"is_service\"] = df[\"type\"].astype(str).str.lower().eq(\"service\")\n",
        "\n",
        "# Keep only non-service and non-empty\n",
        "df_keep = df[~df[\"is_service\"] & ~df[\"is_empty_text\"]].copy()\n",
        "\n",
        "# Add basic lengths\n",
        "df_keep[\"len_chars\"] = df_keep[\"text_raw\"].str.len()\n",
        "df_keep[\"len_words\"] = df_keep[\"text_raw\"].str.split().apply(len)\n",
        "\n",
        "# Stronger filter: keep messages with >=2 words OR question mark\n",
        "df_keep = df_keep[(df_keep[\"len_words\"] >= 2) | (df_keep[\"text_raw\"].str.contains(r\"\\?\", na=False))].copy()\n",
        "\n",
        "print(\"Before:\", len(df), \"| After filter:\", len(df_keep))\n",
        "df_keep[[\"chat_name\",\"from_name\",\"text_raw\"]].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 4 — Clean text + features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 8 — Clean function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "URL_RE = re.compile(r\"\"\"(?i)\\b((?:https?://|www\\.)\\S+)\\b\"\"\")\n",
        "MENTION_RE = re.compile(r\"(?<!\\w)@\\w+\")\n",
        "EMAIL_RE = re.compile(r\"(?i)\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b\")\n",
        "PHONE_RE = re.compile(r\"(?<!\\w)(?:\\+?\\d[\\d\\s\\-().]{6,}\\d)(?!\\w)\")\n",
        "MULTISPACE_RE = re.compile(r\"\\s+\")\n",
        "\n",
        "def clean_text(s: str) -> str:\n",
        "    s = str(s)\n",
        "    s = s.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
        "    s = s.lower()\n",
        "\n",
        "    s = URL_RE.sub(\" <URL> \", s)\n",
        "    s = EMAIL_RE.sub(\" <EMAIL> \", s)\n",
        "    s = MENTION_RE.sub(\" <USER> \", s)\n",
        "    s = PHONE_RE.sub(\" <PHONE> \", s)\n",
        "\n",
        "    # optional: normalize standalone numbers\n",
        "    s = re.sub(r\"\\b\\d+\\b\", \"<NUM>\", s)\n",
        "\n",
        "    s = MULTISPACE_RE.sub(\" \", s).strip()\n",
        "    return s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 9 — Apply cleaning + remove empty after cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "df_work = df_keep.copy()\n",
        "\n",
        "df_work[\"text_clean\"] = df_work[\"text_raw\"].apply(clean_text)\n",
        "\n",
        "df_work[\"clean_len_chars\"] = df_work[\"text_clean\"].str.len()\n",
        "df_work[\"clean_len_words\"] = df_work[\"text_clean\"].str.split().apply(len)\n",
        "\n",
        "df_work = df_work[(df_work[\"clean_len_words\"] >= 2) | (df_work[\"text_clean\"].str.contains(r\"\\?\", na=False))].copy()\n",
        "\n",
        "print(\"Rows after cleaning:\", len(df_work))\n",
        "df_work[[\"text_raw\",\"text_clean\"]].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 10 — Add simple features (useful later)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "df_work[\"has_url\"] = df_work[\"text_clean\"].str.contains(r\"<URL>\", regex=True)\n",
        "df_work[\"has_mention\"] = df_work[\"text_clean\"].str.contains(r\"<USER>\", regex=True)\n",
        "\n",
        "df_work[\"is_question\"] = (\n",
        "    df_work[\"text_raw\"].str.contains(r\"\\?\", na=False) |\n",
        "    df_work[\"text_clean\"].str.startswith((\"why \", \"how \", \"what \", \"when \", \"where \"))\n",
        ")\n",
        "\n",
        "df_work[\"hour\"] = df_work[\"date\"].dt.hour\n",
        "df_work[\"weekday\"] = df_work[\"date\"].dt.weekday\n",
        "\n",
        "# Optional dedup\n",
        "before = len(df_work)\n",
        "df_work = df_work.drop_duplicates(subset=[\"chat_name\",\"from_id\",\"text_clean\"])\n",
        "print(\"Dedup removed:\", before - len(df_work))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 5 — Export clean dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 11 — Export `clean_messages.csv`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "OUT_DIR = Path(\"../data/final\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "KEEP_COLS = [\n",
        "    \"chat_name\",\"msg_id\",\"date\",\"from_name\",\"from_id\",\"type\",\n",
        "    \"text_raw\",\"text_clean\",\n",
        "    \"len_chars\",\"len_words\",\"clean_len_chars\",\"clean_len_words\",\n",
        "    \"has_url\",\"has_mention\",\"is_question\",\"hour\",\"weekday\"\n",
        "]\n",
        "\n",
        "keep_existing = [c for c in KEEP_COLS if c in df_work.columns]\n",
        "df_export = df_work[keep_existing].copy()\n",
        "\n",
        "clean_path = OUT_DIR / \"clean_messages.csv\"\n",
        "df_export.to_csv(clean_path, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(\"Saved:\", clean_path.resolve(), \"| rows:\", len(df_export))\n",
        "df_export.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 5.1 — Priority scoring (optional but recommended)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 12 — Set VIP/Groups + priority score\n",
        "Start with empty sets, run Cell 14 to see top senders/chats, then fill `VIP_PEOPLE`, `IMPORTANT_GROUPS`, `LOW_PRIORITY_GROUPS`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# EDIT THESE LATER (start empty, then fill after you inspect top senders/chats)\n",
        "VIP_PEOPLE = set()\n",
        "IMPORTANT_GROUPS = set()\n",
        "LOW_PRIORITY_GROUPS = set()\n",
        "\n",
        "df_base = df_export.copy()\n",
        "df_base[\"from_name\"] = df_base[\"from_name\"].fillna(\"\")\n",
        "df_base[\"chat_name\"] = df_base[\"chat_name\"].fillna(\"\")\n",
        "df_base[\"text_clean\"] = df_base[\"text_clean\"].fillna(\"\")\n",
        "\n",
        "# simple group heuristic (we can improve later)\n",
        "df_base[\"is_group\"] = df_base[\"chat_name\"].str.contains(r\"(group|чат|канал|channel)\", case=False, regex=True)\n",
        "\n",
        "URGENT_RE = re.compile(r\"\\b(urgent|asap|today|tomorrow|deadline|exam|please|call|help)\\b\", re.I)\n",
        "df_base[\"has_urgent_words\"] = df_base[\"text_clean\"].str.contains(URGENT_RE)\n",
        "df_base[\"is_question_like\"] = df_base[\"text_clean\"].str.contains(r\"\\?\") | df_base[\"text_clean\"].str.startswith((\"why \",\"how \",\"what \",\"when \",\"where \"))\n",
        "\n",
        "df_base[\"is_vip_person\"] = df_base[\"from_name\"].isin(VIP_PEOPLE)\n",
        "df_base[\"is_important_group\"] = df_base[\"chat_name\"].isin(IMPORTANT_GROUPS)\n",
        "df_base[\"is_lowprio_group\"] = df_base[\"chat_name\"].isin(LOW_PRIORITY_GROUPS)\n",
        "\n",
        "df_base[\"priority_score\"] = (\n",
        "    5 * df_base[\"is_vip_person\"].astype(int)\n",
        "    + 3 * df_base[\"is_important_group\"].astype(int)\n",
        "    - 2 * df_base[\"is_lowprio_group\"].astype(int)\n",
        "    + 2 * df_base[\"is_question_like\"].astype(int)\n",
        "    + 2 * df_base[\"has_urgent_words\"].astype(int)\n",
        "    - 1 * df_base[\"is_group\"].astype(int)\n",
        ")\n",
        "\n",
        "df_base[[\"priority_score\",\"chat_name\",\"from_name\",\"text_clean\"]].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 5.2 — Create balanced labeling file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 13 — Balanced sample → `to_label_balanced_1000.csv`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def sample_quota(df, mask, n, seed=42):\n",
        "    pool = df[mask].copy()\n",
        "    if len(pool) == 0:\n",
        "        return pool\n",
        "    return pool.sample(min(n, len(pool)), random_state=seed)\n",
        "\n",
        "N = 1000\n",
        "\n",
        "vip_dm      = sample_quota(df_base, df_base[\"is_vip_person\"] & ~df_base[\"is_group\"], 400)\n",
        "normal_dm   = sample_quota(df_base, ~df_base[\"is_vip_person\"] & ~df_base[\"is_group\"], 250)\n",
        "imp_groups  = sample_quota(df_base, df_base[\"is_important_group\"], 250)\n",
        "noise_group = sample_quota(df_base, df_base[\"is_group\"] & ~df_base[\"is_important_group\"], 100)\n",
        "\n",
        "to_label = pd.concat([vip_dm, normal_dm, imp_groups, noise_group], ignore_index=True)\n",
        "\n",
        "# fallback if your VIP/GROUP lists are empty (very likely now)\n",
        "if len(to_label) < N:\n",
        "    remaining = N - len(to_label)\n",
        "    extra = df_base.sort_values(\"priority_score\", ascending=False).head(remaining)\n",
        "    to_label = pd.concat([to_label, extra], ignore_index=True)\n",
        "\n",
        "to_label = to_label.drop_duplicates(subset=[\"chat_name\",\"from_id\",\"text_clean\"])\n",
        "to_label = to_label.sample(min(N, len(to_label)), random_state=42).copy()\n",
        "\n",
        "to_label[\"label\"] = \"\"  # you fill: important / normal / ignore\n",
        "\n",
        "out_path = OUT_DIR / \"to_label_balanced_1000.csv\"\n",
        "to_label.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(\"Saved:\", out_path.resolve(), \"| rows:\", len(to_label))\n",
        "to_label[[\"priority_score\",\"chat_name\",\"from_name\",\"text_clean\",\"label\"]].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Helper — Choose VIPs and groups"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 14 — Show top senders + top chats"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "print(\"Top chats:\")\n",
        "display(df_export[\"chat_name\"].value_counts().head(25))\n",
        "\n",
        "print(\"\\nTop senders:\")\n",
        "display(df_export[\"from_name\"].value_counts().head(25))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Next steps\n",
        "1. Open `data/final/to_label_balanced_1000.csv`\n",
        "2. Fill `label` with: `important` / `normal` / `ignore`\n",
        "3. Then continue with Step 6: train/val/test split + baseline TF‑IDF model."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}